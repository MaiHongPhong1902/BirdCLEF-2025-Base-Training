{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab42608",
   "metadata": {
    "_cell_guid": "81a54f11-dd84-434f-8426-5f6c70992566",
    "_uuid": "554cbed6-d386-4222-a6a5-d6c4341718dd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:28.271580Z",
     "iopub.status.busy": "2025-04-05T08:38:28.271287Z",
     "iopub.status.idle": "2025-04-05T08:38:32.511396Z",
     "shell.execute_reply": "2025-04-05T08:38:32.510478Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.247124,
     "end_time": "2025-04-05T08:38:32.513420",
     "exception": false,
     "start_time": "2025-04-05T08:38:28.266296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers  --no-index --find-links /kaggle/input/pip-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a615ee8c",
   "metadata": {
    "_cell_guid": "65dacec3-8cac-40a1-873e-61feaa55c223",
    "_uuid": "4bbf7abd-9ad4-4b97-b179-1fcc835a875a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:32.521432Z",
     "iopub.status.busy": "2025-04-05T08:38:32.521192Z",
     "iopub.status.idle": "2025-04-05T08:38:43.768291Z",
     "shell.execute_reply": "2025-04-05T08:38:43.767599Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 11.252378,
     "end_time": "2025-04-05T08:38:43.769749",
     "exception": false,
     "start_time": "2025-04-05T08:38:32.517371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "# Suppress warnings and limit logging output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61f560",
   "metadata": {
    "_cell_guid": "5a80017b-65ff-44c4-90f8-a5a8d060ec2a",
    "_uuid": "9212cbe4-3246-428a-bab4-0e64c375eac6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003058,
     "end_time": "2025-04-05T08:38:43.776326",
     "exception": false,
     "start_time": "2025-04-05T08:38:43.773268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03a99dc",
   "metadata": {
    "_cell_guid": "4037acd7-142b-4595-8fef-bd20e4e89fd7",
    "_uuid": "ad3e67f5-f007-476c-9deb-37e8b19a1b9c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:43.783540Z",
     "iopub.status.busy": "2025-04-05T08:38:43.783305Z",
     "iopub.status.idle": "2025-04-05T08:38:43.845717Z",
     "shell.execute_reply": "2025-04-05T08:38:43.844946Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.06746,
     "end_time": "2025-04-05T08:38:43.846973",
     "exception": false,
     "start_time": "2025-04-05T08:38:43.779513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \"\"\"\n",
    "    Configuration class holding all parameters for the BirdCLEF-2025 inference pipeline.\n",
    "    \"\"\"\n",
    "    # Paths\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_path = '/kaggle/input/birdclef-2025-efficientnet-b0'\n",
    "    \n",
    "    # Audio parameters\n",
    "    FS = 32000\n",
    "    WINDOW_SIZE = 5\n",
    "    \n",
    "    # Mel spectrogram parameters\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 64\n",
    "    N_MELS = 136\n",
    "    FMIN = 20\n",
    "    FMAX = 16000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "    \n",
    "    # Model parameters\n",
    "    model_name = 'efficientnet_b0'\n",
    "    in_channels = 1\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Inference parameters\n",
    "    batch_size = 16\n",
    "    use_tta = False\n",
    "    tta_count = 3\n",
    "    threshold = 0.7\n",
    "    \n",
    "    # Model selection\n",
    "    use_specific_folds = False\n",
    "    folds = [0, 1]\n",
    "    \n",
    "    # Debug options\n",
    "    debug = False\n",
    "    debug_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5829c673",
   "metadata": {
    "_cell_guid": "470f486f-a131-4ba6-b679-054f57125812",
    "_uuid": "c22ab799-9c47-4d0b-a31c-5de7725d41a1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:43.854586Z",
     "iopub.status.busy": "2025-04-05T08:38:43.854355Z",
     "iopub.status.idle": "2025-04-05T08:38:43.860376Z",
     "shell.execute_reply": "2025-04-05T08:38:43.859758Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010992,
     "end_time": "2025-04-05T08:38:43.861523",
     "exception": false,
     "start_time": "2025-04-05T08:38:43.850531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for BirdCLEF-2025 using a timm backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \n",
    "        Args:\n",
    "            cfg: Configuration parameters\n",
    "            num_classes: Number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Create backbone using timm\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=False,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0\n",
    "        )\n",
    "        \n",
    "        # Adjust final layers based on model type\n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.feat_dim = backbone_out\n",
    "        self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Logits for each class\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)\n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        # Apply pooling if needed\n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            \n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dff27e4",
   "metadata": {
    "_cell_guid": "2e76f037-f423-4193-b760-9d469dbf6820",
    "_uuid": "dcd3b7be-d9bf-41c3-99eb-ff9304eddc5b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:43.868822Z",
     "iopub.status.busy": "2025-04-05T08:38:43.868581Z",
     "iopub.status.idle": "2025-04-05T08:38:43.875742Z",
     "shell.execute_reply": "2025-04-05T08:38:43.875110Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012079,
     "end_time": "2025-04-05T08:38:43.876860",
     "exception": false,
     "start_time": "2025-04-05T08:38:43.864781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"\n",
    "    Handles audio processing and mel spectrogram creation.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Initialize with configuration.\n",
    "        \n",
    "        Args:\n",
    "            cfg: Configuration parameters\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "    \n",
    "    def audio2melspec(self, audio_data):\n",
    "        \"\"\"\n",
    "        Convert raw audio data to normalized mel spectrogram.\n",
    "        \n",
    "        Args:\n",
    "            audio_data: 1D numpy array of audio samples\n",
    "            \n",
    "        Returns:\n",
    "            Normalized mel spectrogram\n",
    "        \"\"\"\n",
    "        # Handle NaN values if present\n",
    "        if np.isnan(audio_data).any():\n",
    "            mean_signal = np.nanmean(audio_data)\n",
    "            audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "        \n",
    "        # Create mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio_data,\n",
    "            sr=self.cfg.FS,\n",
    "            n_fft=self.cfg.N_FFT,\n",
    "            hop_length=self.cfg.HOP_LENGTH,\n",
    "            n_mels=self.cfg.N_MELS,\n",
    "            fmin=self.cfg.FMIN,\n",
    "            fmax=self.cfg.FMAX,\n",
    "            power=2.0\n",
    "        )\n",
    "        \n",
    "        # Convert to dB scale and normalize\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "        \n",
    "        return mel_spec_norm\n",
    "    \n",
    "    def process_audio_segment(self, audio_data):\n",
    "        \"\"\"\n",
    "        Process an audio segment to obtain a mel spectrogram with the target shape.\n",
    "        \n",
    "        Args:\n",
    "            audio_data: 1D numpy array of audio samples\n",
    "            \n",
    "        Returns:\n",
    "            Processed mel spectrogram as a float32 numpy array\n",
    "        \"\"\"\n",
    "        # Pad audio if needed\n",
    "        if len(audio_data) < self.cfg.FS * self.cfg.WINDOW_SIZE:\n",
    "            audio_data = np.pad(\n",
    "                audio_data,\n",
    "                (0, self.cfg.FS * self.cfg.WINDOW_SIZE - len(audio_data)),\n",
    "                mode='constant'\n",
    "            )\n",
    "        \n",
    "        # Generate mel spectrogram\n",
    "        mel_spec = self.audio2melspec(audio_data)\n",
    "        \n",
    "        # Resize to target shape if needed\n",
    "        if mel_spec.shape != self.cfg.TARGET_SHAPE:\n",
    "            mel_spec = cv2.resize(mel_spec, self.cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        return mel_spec.astype(np.float32)\n",
    "    \n",
    "    def apply_tta(self, spec, tta_idx):\n",
    "        \"\"\"\n",
    "        Apply test-time augmentation to spectrogram.\n",
    "        \n",
    "        Args:\n",
    "            spec: Input mel spectrogram\n",
    "            tta_idx: Index indicating which augmentation to apply\n",
    "            \n",
    "        Returns:\n",
    "            Augmented spectrogram\n",
    "        \"\"\"\n",
    "        if tta_idx == 0:\n",
    "            # No augmentation\n",
    "            return spec\n",
    "        elif tta_idx == 1:\n",
    "            # Time shift (horizontal flip)\n",
    "            return np.flip(spec, axis=1)\n",
    "        elif tta_idx == 2:\n",
    "            # Frequency shift (vertical flip)\n",
    "            return np.flip(spec, axis=0)\n",
    "        else:\n",
    "            return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0f45a4",
   "metadata": {
    "_cell_guid": "09f2604a-ff50-4b62-bf9c-d85b2d4e5bc1",
    "_uuid": "72765723-9a43-4d83-bf17-7334abc36bbd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:43.883909Z",
     "iopub.status.busy": "2025-04-05T08:38:43.883670Z",
     "iopub.status.idle": "2025-04-05T08:38:43.891517Z",
     "shell.execute_reply": "2025-04-05T08:38:43.890696Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012885,
     "end_time": "2025-04-05T08:38:43.892883",
     "exception": false,
     "start_time": "2025-04-05T08:38:43.879998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"\n",
    "    Handles model loading and inference.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        \"\"\"\n",
    "        Initialize with configuration.\n",
    "        \n",
    "        Args:\n",
    "            cfg: Configuration parameters\n",
    "            num_classes: Number of output classes\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "        self.num_classes = num_classes\n",
    "        self.models = []\n",
    "    \n",
    "    def find_model_files(self):\n",
    "        \"\"\"\n",
    "        Find all .pth model files in the specified model directory.\n",
    "        \n",
    "        Returns:\n",
    "            List of model file paths\n",
    "        \"\"\"\n",
    "        model_files = []\n",
    "        model_dir = Path(self.cfg.model_path)\n",
    "        for path in model_dir.glob('**/*.pth'):\n",
    "            model_files.append(str(path))\n",
    "        return model_files\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"\n",
    "        Load all found model files and prepare them for ensemble inference.\n",
    "        \n",
    "        Returns:\n",
    "            List of loaded PyTorch models\n",
    "        \"\"\"\n",
    "        model_files = self.find_model_files()\n",
    "        if not model_files:\n",
    "            print(f\"Warning: No model files found under {self.cfg.model_path}!\")\n",
    "            return self.models\n",
    "        \n",
    "        print(f\"Found a total of {len(model_files)} model files.\")\n",
    "        \n",
    "        # Filter for specific folds if required\n",
    "        if self.cfg.use_specific_folds:\n",
    "            filtered_files = []\n",
    "            for fold in self.cfg.folds:\n",
    "                fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "                filtered_files.extend(fold_files)\n",
    "            model_files = filtered_files\n",
    "            print(f\"Using {len(model_files)} model files for the specified folds ({self.cfg.folds}).\")\n",
    "        \n",
    "        # Load each model\n",
    "        for model_path in model_files:\n",
    "            try:\n",
    "                print(f\"Loading model: {model_path}\")\n",
    "                checkpoint = torch.load(model_path, map_location=self.cfg.device)\n",
    "                model = BirdCLEFModel(self.cfg, self.num_classes)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                model = model.to(self.cfg.device)\n",
    "                model.eval()\n",
    "                self.models.append(model)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model {model_path}: {e}\")\n",
    "        \n",
    "        return self.models\n",
    "    \n",
    "    def predict_batch(self, mel_spec_tensor, tta_idx=None):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of spectrograms.\n",
    "        \n",
    "        Args:\n",
    "            mel_spec_tensor: Tensor of mel spectrograms\n",
    "            tta_idx: Index for test-time augmentation\n",
    "            \n",
    "        Returns:\n",
    "            Predictions for the batch\n",
    "        \"\"\"\n",
    "        if len(self.models) == 1:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.models[0](mel_spec_tensor)\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                return probs\n",
    "        else:\n",
    "            # Ensemble prediction\n",
    "            all_preds = []\n",
    "            for model in self.models:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(mel_spec_tensor)\n",
    "                    probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                    all_preds.append(probs)\n",
    "            return np.mean(all_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89c889a",
   "metadata": {
    "_cell_guid": "a3b91847-20a7-4886-9112-fa5d8d64291b",
    "_uuid": "d16ec2a5-d594-41a2-bba8-56533d810125",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:43.900164Z",
     "iopub.status.busy": "2025-04-05T08:38:43.899946Z",
     "iopub.status.idle": "2025-04-05T08:38:43.931251Z",
     "shell.execute_reply": "2025-04-05T08:38:43.930480Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.036386,
     "end_time": "2025-04-05T08:38:43.932421",
     "exception": false,
     "start_time": "2025-04-05T08:38:43.896035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdCLEF2025Pipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline for BirdCLEF-2025 inference task.\n",
    "    \n",
    "    Organizes the complete inference process:\n",
    "    - Loading taxonomy data\n",
    "    - Loading models\n",
    "    - Processing audio files\n",
    "    - Making predictions\n",
    "    - Creating submission file\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Args:\n",
    "            cfg: Configuration object with parameters\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "        self.taxonomy_df = None\n",
    "        self.species_ids = []\n",
    "        \n",
    "        # Initialize components\n",
    "        self._load_taxonomy()\n",
    "        self.audio_processor = AudioProcessor(cfg)\n",
    "        self.model_manager = ModelManager(cfg, len(self.species_ids))\n",
    "    \n",
    "    def _load_taxonomy(self):\n",
    "        \"\"\"\n",
    "        Load taxonomy data from CSV and extract species identifiers.\n",
    "        \"\"\"\n",
    "        print(\"Loading taxonomy data...\")\n",
    "        self.taxonomy_df = pd.read_csv(self.cfg.taxonomy_csv)\n",
    "        self.species_ids = self.taxonomy_df['primary_label'].tolist()\n",
    "        print(f\"Number of classes: {len(self.species_ids)}\")\n",
    "    \n",
    "    def predict_on_spectrogram(self, audio_path):\n",
    "        \"\"\"\n",
    "        Process a single audio file and predict species presence for each segment.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to the audio file\n",
    "            \n",
    "        Returns:\n",
    "            Tuple (row_ids, predictions) for each segment\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        row_ids = []\n",
    "        soundscape_id = Path(audio_path).stem\n",
    "        \n",
    "        try:\n",
    "            print(f\"Processing {soundscape_id}\")\n",
    "            audio_data, _ = librosa.load(audio_path, sr=self.cfg.FS)\n",
    "            total_segments = int(len(audio_data) / (self.cfg.FS * self.cfg.WINDOW_SIZE))\n",
    "            \n",
    "            for segment_idx in range(total_segments):\n",
    "                # Extract segment\n",
    "                start_sample = segment_idx * self.cfg.FS * self.cfg.WINDOW_SIZE\n",
    "                end_sample = start_sample + self.cfg.FS * self.cfg.WINDOW_SIZE\n",
    "                segment_audio = audio_data[start_sample:end_sample]\n",
    "                \n",
    "                # Create row ID\n",
    "                end_time_sec = (segment_idx + 1) * self.cfg.WINDOW_SIZE\n",
    "                row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "                row_ids.append(row_id)\n",
    "                \n",
    "                if self.cfg.use_tta:\n",
    "                    all_preds = []\n",
    "                    for tta_idx in range(self.cfg.tta_count):\n",
    "                        # Process with TTA\n",
    "                        mel_spec = self.audio_processor.process_audio_segment(segment_audio)\n",
    "                        mel_spec = self.audio_processor.apply_tta(mel_spec, tta_idx)\n",
    "                        mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                        mel_spec_tensor = mel_spec_tensor.to(self.cfg.device)\n",
    "                        \n",
    "                        # Get predictions\n",
    "                        probs = self.model_manager.predict_batch(mel_spec_tensor)\n",
    "                        all_preds.append(probs.squeeze())\n",
    "                    \n",
    "                    final_preds = np.mean(all_preds, axis=0)\n",
    "                else:\n",
    "                    # Process without TTA\n",
    "                    mel_spec = self.audio_processor.process_audio_segment(segment_audio)\n",
    "                    mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                    mel_spec_tensor = mel_spec_tensor.to(self.cfg.device)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    final_preds = self.model_manager.predict_batch(mel_spec_tensor).squeeze()\n",
    "                \n",
    "                predictions.append(final_preds)\n",
    "                \n",
    "                # Free memory\n",
    "                if segment_idx % 10 == 0:\n",
    "                    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                    gc.collect()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "        \n",
    "        return row_ids, predictions\n",
    "    \n",
    "    def plot_prediction_heatmap(self, row_ids, predictions):\n",
    "        \"\"\"\n",
    "        Plot heatmap of predictions across species and time segments.\n",
    "        \n",
    "        Args:\n",
    "            row_ids: List of row IDs\n",
    "            predictions: List of prediction arrays\n",
    "        \"\"\"\n",
    "        # Get time points and create labels\n",
    "        time_points = [int(row_id.split('_')[-1]) for row_id in row_ids]\n",
    "        time_labels = [f\"{t}s\" for t in time_points]\n",
    "        \n",
    "        # Find the species with any detection above threshold\n",
    "        species_with_detections = []\n",
    "        species_indices = []\n",
    "        \n",
    "        predictions_array = np.array(predictions)\n",
    "        max_probs_per_species = np.max(predictions_array, axis=0)\n",
    "        \n",
    "        # Get species that have any segment above threshold\n",
    "        for i, max_prob in enumerate(max_probs_per_species):\n",
    "            if max_prob > self.cfg.threshold * 0.5:  # Use lower threshold for display\n",
    "                species_indices.append(i)\n",
    "                species_with_detections.append(self.species_ids[i])\n",
    "        \n",
    "        # If too many species, limit to top ones\n",
    "        if len(species_with_detections) > 20:\n",
    "            top_indices = np.argsort(max_probs_per_species)[-20:]\n",
    "            species_indices = [i for i in species_indices if i in top_indices]\n",
    "            species_with_detections = [self.species_ids[i] for i in species_indices]\n",
    "        \n",
    "        # If no species above threshold, show top 10\n",
    "        if not species_with_detections:\n",
    "            top_indices = np.argsort(max_probs_per_species)[-10:]\n",
    "            species_indices = list(top_indices)\n",
    "            species_with_detections = [self.species_ids[i] for i in species_indices]\n",
    "        \n",
    "        # Create heatmap data\n",
    "        heatmap_data = np.zeros((len(species_with_detections), len(time_points)))\n",
    "        for i, species_idx in enumerate(species_indices):\n",
    "            for j, pred in enumerate(predictions):\n",
    "                heatmap_data[i, j] = pred[species_idx]\n",
    "        \n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(14, max(6, len(species_with_detections) * 0.4)))\n",
    "        plt.imshow(heatmap_data, aspect='auto', cmap='viridis')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('Detection Probability')\n",
    "        \n",
    "        # Add threshold line on colorbar\n",
    "        cbar.ax.axhline(y=self.cfg.threshold * cbar.ax.get_ylim()[1], color='r', linestyle='--')\n",
    "        \n",
    "        # Add labels\n",
    "        plt.title('Bird Species Detection Heatmap')\n",
    "        plt.xlabel('Time Segment')\n",
    "        plt.ylabel('Species')\n",
    "        \n",
    "        # Set tick labels\n",
    "        plt.yticks(np.arange(len(species_with_detections)), species_with_detections)\n",
    "        \n",
    "        # Show fewer x-tick labels if there are many\n",
    "        if len(time_labels) > 20:\n",
    "            step = len(time_labels) // 10\n",
    "            plt.xticks(np.arange(0, len(time_labels), step), time_labels[::step])\n",
    "        else:\n",
    "            plt.xticks(np.arange(len(time_labels)), time_labels)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict_file(self, audio_path, visualize=True, return_results=False):\n",
    "        \"\"\"\n",
    "        Run prediction on a single audio file and optionally visualize the results.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to the audio file to predict\n",
    "            visualize: Whether to visualize the results\n",
    "            return_results: Whether to return detailed results\n",
    "            \n",
    "        Returns:\n",
    "            If return_results is True, returns a tuple (row_ids, predictions, detected_species)\n",
    "        \"\"\"\n",
    "        # Ensure models are loaded\n",
    "        if not self.model_manager.models:\n",
    "            self.model_manager.load_models()\n",
    "            if not self.model_manager.models:\n",
    "                print(\"No models found! Please check model paths.\")\n",
    "                return None\n",
    "        \n",
    "        # Run prediction\n",
    "        row_ids, predictions = self.predict_on_spectrogram(audio_path)\n",
    "        \n",
    "        # Process results\n",
    "        detected_species = {}\n",
    "        print(f\"\\nPrediction results for {Path(audio_path).name}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for row_id, pred in zip(row_ids, predictions):\n",
    "            time_sec = int(row_id.split('_')[-1])\n",
    "            # Find top predictions above threshold\n",
    "            above_threshold = [(i, p) for i, p in enumerate(pred) if p > self.cfg.threshold]\n",
    "            above_threshold.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            if above_threshold:\n",
    "                detected_for_segment = []\n",
    "                for idx, prob in above_threshold:\n",
    "                    species = self.species_ids[idx]\n",
    "                    detected_for_segment.append((species, prob))\n",
    "                    # Update overall detected species\n",
    "                    if species not in detected_species:\n",
    "                        detected_species[species] = []\n",
    "                    detected_species[species].append((time_sec, prob))\n",
    "                \n",
    "                # Print top detections for this segment\n",
    "                species_str = \", \".join([f\"{s} ({p:.3f})\" for s, p in detected_for_segment[:3]])\n",
    "                print(f\"Time {time_sec}s: {species_str}\")\n",
    "        \n",
    "        # Summarize results\n",
    "        print(\"-\" * 80)\n",
    "        if detected_species:\n",
    "            print(\"\\nSummary of detected species:\")\n",
    "            # Sort species by highest confidence\n",
    "            sorted_species = sorted(\n",
    "                detected_species.items(), \n",
    "                key=lambda x: max([p for _, p in x[1]]), \n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            for species, detections in sorted_species:\n",
    "                max_prob = max([p for _, p in detections])\n",
    "                count = len(detections)\n",
    "                print(f\"{species}: detected in {count} segments, max probability {max_prob:.3f}\")\n",
    "        else:\n",
    "            print(\"No species detected above threshold.\")\n",
    "        \n",
    "        # Visualize if requested\n",
    "        if visualize:\n",
    "            self.visualize_predictions(audio_path, row_ids, predictions)\n",
    "        \n",
    "        if return_results:\n",
    "            return row_ids, predictions, detected_species\n",
    "    \n",
    "    def visualize_predictions(self, audio_path, row_ids, predictions):\n",
    "        \"\"\"\n",
    "        Visualize prediction results for an audio file.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to the audio file\n",
    "            row_ids: List of row IDs from prediction\n",
    "            predictions: List of prediction arrays\n",
    "        \"\"\"\n",
    "        # Create visualizer\n",
    "        visualizer = DataVisualizer(self.cfg)\n",
    "        \n",
    "        # Plot audio waveform for context\n",
    "        try:\n",
    "            audio_data, _ = librosa.load(audio_path, sr=self.cfg.FS)\n",
    "            visualizer.plot_waveform(audio_data, title=f\"Audio Waveform: {Path(audio_path).stem}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not visualize waveform: {e}\")\n",
    "        \n",
    "        # Get time points\n",
    "        time_points = [int(row_id.split('_')[-1]) for row_id in row_ids]\n",
    "        \n",
    "        # Find species with highest probability overall\n",
    "        all_probs = np.max([pred for pred in predictions], axis=0)\n",
    "        top_species_idx = np.argsort(all_probs)[-10:]  # Get top 10 species\n",
    "        \n",
    "        # Plot prediction results\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Plot each top species\n",
    "        for idx in reversed(top_species_idx):  # Reverse to show highest probability first\n",
    "            species = self.species_ids[idx]\n",
    "            species_probs = [pred[idx] for pred in predictions]\n",
    "            plt.plot(time_points, species_probs, label=species, linewidth=2)\n",
    "        \n",
    "        # Add threshold line\n",
    "        plt.axhline(y=self.cfg.threshold, color='r', linestyle='--', alpha=0.7,\n",
    "                   label=f\"Detection threshold ({self.cfg.threshold})\")\n",
    "        \n",
    "        # Add labels and legend\n",
    "        plt.xlabel(\"Time (seconds)\")\n",
    "        plt.ylabel(\"Detection Probability\")\n",
    "        plt.title(\"Bird Species Detection Probabilities Over Time\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Format tick labels\n",
    "        plt.xticks(time_points[::max(1, len(time_points)//10)])  # Show ~10 tick labels\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Plot heatmap of all predictions\n",
    "        self.plot_prediction_heatmap(row_ids, predictions)\n",
    "    \n",
    "    def run_inference(self):\n",
    "        \"\"\"\n",
    "        Run inference on all test soundscape audio files.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple (all_row_ids, all_predictions) aggregated from all files\n",
    "        \"\"\"\n",
    "        test_files = list(Path(self.cfg.test_soundscapes).glob('*.ogg'))\n",
    "        if self.cfg.debug:\n",
    "            print(f\"Debug mode enabled, using only {self.cfg.debug_count} files\")\n",
    "            test_files = test_files[:self.cfg.debug_count]\n",
    "        print(f\"Found {len(test_files)} test soundscapes\")\n",
    "        \n",
    "        all_row_ids = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        for audio_path in tqdm(test_files):\n",
    "            row_ids, predictions = self.predict_on_spectrogram(str(audio_path))\n",
    "            all_row_ids.extend(row_ids)\n",
    "            all_predictions.extend(predictions)\n",
    "            \n",
    "            # Free memory\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            gc.collect()\n",
    "        \n",
    "        return all_row_ids, all_predictions\n",
    "    \n",
    "    def create_submission(self, row_ids, predictions):\n",
    "        \"\"\"\n",
    "        Create the submission dataframe based on predictions.\n",
    "        \n",
    "        Args:\n",
    "            row_ids: List of row identifiers for each segment\n",
    "            predictions: List of prediction arrays\n",
    "            \n",
    "        Returns:\n",
    "            A pandas DataFrame formatted for submission\n",
    "        \"\"\"\n",
    "        print(\"Creating submission dataframe...\")\n",
    "        submission_dict = {'row_id': row_ids}\n",
    "        \n",
    "        for i, species in enumerate(self.species_ids):\n",
    "            submission_dict[species] = [pred[i] for pred in predictions]\n",
    "        \n",
    "        submission_df = pd.DataFrame(submission_dict)\n",
    "        submission_df.set_index('row_id', inplace=True)\n",
    "        \n",
    "        # Ensure all required columns are present\n",
    "        sample_sub = pd.read_csv(self.cfg.submission_csv, index_col='row_id')\n",
    "        missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "            for col in missing_cols:\n",
    "                submission_df[col] = 0.0\n",
    "        \n",
    "        # Match column order with sample submission\n",
    "        submission_df = submission_df[sample_sub.columns]\n",
    "        submission_df = submission_df.reset_index()\n",
    "        \n",
    "        return submission_df\n",
    "    \n",
    "    def smooth_submission(self, submission_path):\n",
    "        \"\"\"\n",
    "        Post-process the submission CSV by smoothing predictions for temporal consistency.\n",
    "        \n",
    "        Args:\n",
    "            submission_path: Path to the submission CSV file\n",
    "        \"\"\"\n",
    "        print(\"Smoothing submission predictions...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        cols = sub.columns[1:]\n",
    "        \n",
    "        # Extract group names by splitting row_id\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            idx = np.where(groups == group)[0]\n",
    "            predictions = sub.iloc[idx][cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # Apply smoothing with neighboring segments\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                \n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "            \n",
    "            # Update the submission dataframe\n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"Smoothed submission saved to {submission_path}\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Main method to execute the complete inference pipeline.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(\"Starting BirdCLEF-2025 inference...\")\n",
    "        print(f\"Using device: {self.cfg.device}\")\n",
    "        print(f\"TTA enabled: {self.cfg.use_tta} (variations: {self.cfg.tta_count if self.cfg.use_tta else 0})\")\n",
    "        \n",
    "        # Load models\n",
    "        self.model_manager.load_models()\n",
    "        if not self.model_manager.models:\n",
    "            print(\"No models found! Please check model paths.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Model usage: {'Single model' if len(self.model_manager.models) == 1 else f'Ensemble of {len(self.model_manager.models)} models'}\")\n",
    "        \n",
    "        # Run inference\n",
    "        row_ids, predictions = self.run_inference()\n",
    "        \n",
    "        # Create submission\n",
    "        submission_df = self.create_submission(row_ids, predictions)\n",
    "        submission_path = 'submission.csv'\n",
    "        submission_df.to_csv(submission_path, index=False)\n",
    "        print(f\"Initial submission saved to {submission_path}\")\n",
    "        \n",
    "        # Apply smoothing\n",
    "        self.smooth_submission(submission_path)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Inference completed in {(end_time - start_time) / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c4eb60f",
   "metadata": {
    "_cell_guid": "f798d110-fa9d-47bc-bfb7-610f4bd155cd",
    "_uuid": "760499a1-ed1e-4d25-a35e-bfe851bb8eff",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:43.939716Z",
     "iopub.status.busy": "2025-04-05T08:38:43.939483Z",
     "iopub.status.idle": "2025-04-05T08:38:43.951387Z",
     "shell.execute_reply": "2025-04-05T08:38:43.950589Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.016942,
     "end_time": "2025-04-05T08:38:43.952597",
     "exception": false,
     "start_time": "2025-04-05T08:38:43.935655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataVisualizer:\n",
    "    \"\"\"\n",
    "    Handles visualization of audio data, spectrograms, and predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Initialize with configuration.\n",
    "        \n",
    "        Args:\n",
    "            cfg: Configuration parameters\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "        self.audio_processor = AudioProcessor(cfg)\n",
    "    \n",
    "    def plot_waveform(self, audio_data, title=\"Audio Waveform\", figsize=(12, 4)):\n",
    "        \"\"\"\n",
    "        Plot audio waveform.\n",
    "        \n",
    "        Args:\n",
    "            audio_data: 1D numpy array of audio samples\n",
    "            title: Plot title\n",
    "            figsize: Figure size\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        time_axis = np.arange(0, len(audio_data)) / self.cfg.FS\n",
    "        plt.plot(time_axis, audio_data)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_spectrogram(self, mel_spec, title=\"Mel Spectrogram\", figsize=(10, 6)):\n",
    "        \"\"\"\n",
    "        Plot a mel spectrogram.\n",
    "        \n",
    "        Args:\n",
    "            mel_spec: 2D numpy array of mel spectrogram\n",
    "            title: Plot title\n",
    "            figsize: Figure size\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(mel_spec, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time Frames\")\n",
    "        plt.ylabel(\"Mel Frequency Bands\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def preview_audio_file(self, audio_path, segment_idx=0, show_spectrogram=True):\n",
    "        \"\"\"\n",
    "        Preview audio file with waveform and spectrogram visualization.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            segment_idx: Segment index to visualize\n",
    "            show_spectrogram: Whether to show the spectrogram\n",
    "        \"\"\"\n",
    "        try:\n",
    "            audio_data, _ = librosa.load(audio_path, sr=self.cfg.FS)\n",
    "            \n",
    "            # Show full audio waveform\n",
    "            soundscape_id = Path(audio_path).stem\n",
    "            self.plot_waveform(audio_data, \n",
    "                             title=f\"Full Audio Waveform: {soundscape_id} ({len(audio_data)/self.cfg.FS:.2f}s)\")\n",
    "            \n",
    "            # Extract and show specific segment\n",
    "            if segment_idx >= 0:\n",
    "                total_segments = int(len(audio_data) / (self.cfg.FS * self.cfg.WINDOW_SIZE))\n",
    "                if segment_idx >= total_segments:\n",
    "                    print(f\"Warning: Segment index {segment_idx} exceeds total segments {total_segments}.\")\n",
    "                    segment_idx = total_segments - 1\n",
    "                \n",
    "                start_sample = segment_idx * self.cfg.FS * self.cfg.WINDOW_SIZE\n",
    "                end_sample = start_sample + self.cfg.FS * self.cfg.WINDOW_SIZE\n",
    "                segment_audio = audio_data[start_sample:end_sample]\n",
    "                \n",
    "                # Show segment waveform\n",
    "                self.plot_waveform(segment_audio, \n",
    "                                 title=f\"Segment {segment_idx} Waveform ({self.cfg.WINDOW_SIZE}s)\")\n",
    "                \n",
    "                # Show segment spectrogram\n",
    "                if show_spectrogram:\n",
    "                    mel_spec = self.audio_processor.process_audio_segment(segment_audio)\n",
    "                    self.plot_spectrogram(mel_spec, \n",
    "                                       title=f\"Segment {segment_idx} Mel Spectrogram\")\n",
    "                    \n",
    "                    print(f\"Mel spectrogram shape: {mel_spec.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error previewing audio file {audio_path}: {e}\")\n",
    "    \n",
    "    def preview_predictions(self, audio_path, pipeline, top_k=5):\n",
    "        \"\"\"\n",
    "        Run predictions on an audio file and visualize the results.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            pipeline: BirdCLEF pipeline instance\n",
    "            top_k: Number of top predictions to show\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        row_ids, predictions = pipeline.predict_on_spectrogram(audio_path)\n",
    "        \n",
    "        # Plot predictions over time\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Get all unique species that appear in top-k across all segments\n",
    "        top_species_indices = set()\n",
    "        for pred in predictions:\n",
    "            top_k_indices = np.argsort(pred)[-top_k:]\n",
    "            top_species_indices.update(top_k_indices)\n",
    "        \n",
    "        top_species_indices = list(top_species_indices)\n",
    "        top_species_names = [pipeline.species_ids[i] for i in top_species_indices]\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        segment_times = [int(row_id.split('_')[-1]) for row_id in row_ids]\n",
    "        pred_values = np.zeros((len(top_species_indices), len(predictions)))\n",
    "        \n",
    "        for i, species_idx in enumerate(top_species_indices):\n",
    "            for j, pred in enumerate(predictions):\n",
    "                pred_values[i, j] = pred[species_idx]\n",
    "        \n",
    "        # Plot each species predictions over time\n",
    "        for i, species_idx in enumerate(top_species_indices):\n",
    "            species_name = pipeline.species_ids[species_idx]\n",
    "            plt.plot(segment_times, pred_values[i], label=species_name)\n",
    "        \n",
    "        plt.xlabel(\"Time (seconds)\")\n",
    "        plt.ylabel(\"Prediction Probability\")\n",
    "        plt.title(\"Bird Species Predictions Over Time\")\n",
    "        plt.axhline(y=pipeline.cfg.threshold, color='r', linestyle='--', label=f\"Threshold ({pipeline.cfg.threshold})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display highest prediction per segment\n",
    "        print(\"\\nTop prediction per segment:\")\n",
    "        for i, (row_id, pred) in enumerate(zip(row_ids, predictions)):\n",
    "            time_sec = int(row_id.split('_')[-1])\n",
    "            top_idx = np.argmax(pred)\n",
    "            top_species = pipeline.species_ids[top_idx]\n",
    "            top_score = pred[top_idx]\n",
    "            detected = \"DETECTED\" if top_score > pipeline.cfg.threshold else \"below threshold\"\n",
    "            print(f\"Time {time_sec}s: {top_species} ({top_score:.4f}) - {detected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c683f222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:43.959699Z",
     "iopub.status.busy": "2025-04-05T08:38:43.959472Z",
     "iopub.status.idle": "2025-04-05T08:38:43.964997Z",
     "shell.execute_reply": "2025-04-05T08:38:43.964221Z"
    },
    "papermill": {
     "duration": 0.010413,
     "end_time": "2025-04-05T08:38:43.966196",
     "exception": false,
     "start_time": "2025-04-05T08:38:43.955783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preview_dataset(cfg):\n",
    "    \"\"\"\n",
    "    Preview the dataset by showing audio examples and taxonomy information.\n",
    "    \n",
    "    Args:\n",
    "        cfg: Configuration parameters\n",
    "    \"\"\"\n",
    "    # Load taxonomy data\n",
    "    print(\"Loading taxonomy data...\")\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "    \n",
    "    # Print taxonomy summary\n",
    "    print(f\"\\nTaxonomy Summary:\")\n",
    "    print(f\"Total bird species: {len(taxonomy_df)}\")\n",
    "    \n",
    "    # Display sample of taxonomy data\n",
    "    print(\"\\nSample taxonomy data:\")\n",
    "    print(taxonomy_df.head())\n",
    "    \n",
    "    # Show distribution of species by order\n",
    "    if 'order' in taxonomy_df.columns:\n",
    "        order_counts = taxonomy_df['order'].value_counts()\n",
    "        print(\"\\nSpecies distribution by order:\")\n",
    "        print(order_counts)\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        order_counts.plot(kind='bar')\n",
    "        plt.title('Number of Species by Order')\n",
    "        plt.xlabel('Order')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Preview test soundscapes\n",
    "    print(\"\\nPreview test soundscapes:\")\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    if not test_files:\n",
    "        print(\"No test files found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscape files\")\n",
    "    \n",
    "    # Show sample audio file\n",
    "    if len(test_files) > 0:\n",
    "        sample_file = test_files[0]\n",
    "        print(f\"\\nPreviewing sample file: {sample_file}\")\n",
    "        \n",
    "        visualizer = DataVisualizer(cfg)\n",
    "        visualizer.preview_audio_file(str(sample_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d71f48c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:38:43.973405Z",
     "iopub.status.busy": "2025-04-05T08:38:43.973183Z",
     "iopub.status.idle": "2025-04-05T08:38:48.193432Z",
     "shell.execute_reply": "2025-04-05T08:38:48.192456Z"
    },
    "papermill": {
     "duration": 4.22547,
     "end_time": "2025-04-05T08:38:48.194955",
     "exception": false,
     "start_time": "2025-04-05T08:38:43.969485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading taxonomy data...\n",
      "\n",
      "Taxonomy Summary:\n",
      "Total bird species: 206\n",
      "\n",
      "Sample taxonomy data:\n",
      "  primary_label  inat_taxon_id               scientific_name  \\\n",
      "0       1139490        1139490          Ragoniella pulchella   \n",
      "1       1192948        1192948         Oxyprora surinamensis   \n",
      "2       1194042        1194042           Copiphora colombiae   \n",
      "3        126247         126247       Leptodactylus insularum   \n",
      "4       1346504        1346504  Neoconocephalus brachypterus   \n",
      "\n",
      "                    common_name class_name  \n",
      "0          Ragoniella pulchella    Insecta  \n",
      "1         Oxyprora surinamensis    Insecta  \n",
      "2           Copiphora colombiae    Insecta  \n",
      "3        Spotted Foam-nest Frog   Amphibia  \n",
      "4  Neoconocephalus brachypterus    Insecta  \n",
      "\n",
      "Preview test soundscapes:\n",
      "No test files found!\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n",
      "\n",
      "Running full inference pipeline...\n",
      "Starting BirdCLEF-2025 inference...\n",
      "Using device: cuda\n",
      "TTA enabled: False (variations: 0)\n",
      "Found a total of 5 model files.\n",
      "Loading model: /kaggle/input/birdclef-2025-efficientnet-b0/model_fold0.pth\n",
      "Loading model: /kaggle/input/birdclef-2025-efficientnet-b0/model_fold3.pth\n",
      "Loading model: /kaggle/input/birdclef-2025-efficientnet-b0/model_fold1.pth\n",
      "Loading model: /kaggle/input/birdclef-2025-efficientnet-b0/model_fold2.pth\n",
      "Loading model: /kaggle/input/birdclef-2025-efficientnet-b0/model_fold4.pth\n",
      "Model usage: Ensemble of 5 models\n",
      "Found 0 test soundscapes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea823d50482421eadb2db580de18f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission dataframe...\n",
      "Initial submission saved to submission.csv\n",
      "Smoothing submission predictions...\n",
      "Smoothed submission saved to submission.csv\n",
      "Inference completed in 0.07 minutes\n"
     ]
    }
   ],
   "source": [
    "# Run the BirdCLEF2025 Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = CFG()\n",
    "    print(f\"Using device: {cfg.device}\")\n",
    "    \n",
    "    # Preview the dataset structure\n",
    "    preview_dataset(cfg)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = BirdCLEF2025Pipeline(cfg)\n",
    "    \n",
    "    # Example: Predict a single file with visualizations\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    if test_files and len(test_files) > 0:\n",
    "        print(\"\\nRunning prediction on a sample file...\")\n",
    "        pipeline.predict_file(str(test_files[0]))\n",
    "    \n",
    "    # Run full pipeline for all files\n",
    "    print(\"\\nRunning full inference pipeline...\")\n",
    "    pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 6902504,
     "sourceId": 11075449,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6959095,
     "sourceId": 11153836,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24.209546,
   "end_time": "2025-04-05T08:38:49.919710",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-05T08:38:25.710164",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "20337234ab624e80a59e474e6cbb94b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_280480a79af04e2d998330dea939a8a7",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fa7be9aba2924055bf1b269406c751e8",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0
      }
     },
     "280480a79af04e2d998330dea939a8a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "35329c2d6865409c842db3f9324c707b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a3f3101094ef40d6845704ab5c1110fb",
       "placeholder": "​",
       "style": "IPY_MODEL_f4b96b38edc24d4991aaf11e12594d92",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "4cfa71f6327645af836f306649115118": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ea823d50482421eadb2db580de18f9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ba5bd7c4a9224fa4830987c091b0ccc8",
        "IPY_MODEL_20337234ab624e80a59e474e6cbb94b9",
        "IPY_MODEL_35329c2d6865409c842db3f9324c707b"
       ],
       "layout": "IPY_MODEL_d1b3380a6fa64b30961d5a237a86a4ec",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8ab758ce7cc04069a8a095954010fd83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a3f3101094ef40d6845704ab5c1110fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ba5bd7c4a9224fa4830987c091b0ccc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4cfa71f6327645af836f306649115118",
       "placeholder": "​",
       "style": "IPY_MODEL_8ab758ce7cc04069a8a095954010fd83",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "d1b3380a6fa64b30961d5a237a86a4ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4b96b38edc24d4991aaf11e12594d92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fa7be9aba2924055bf1b269406c751e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
